---
title: "Competitive Analysis Through Topic Modeling"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---


## Background 

The VP of US eCommerce at *Transcend* (a Taiwanese electronics company) has witnessed sales for its SD card on Amazon eroding from 2013 to 2015 (the “present”). The VP says that while star score has increased, sales have been dropping. The VP says that he thinks Transcend has been the sales leader on Amazon since it joined Amazon in 2010. 

He has approached your team to diagnose what may be going on in the Amazon ecosystem, and to start with competitive analysis. 

```{r}

# load in data 

setwd('/Users/Alex.Shypula@ibm.com/Documents/Other/Tambe')

electronics <- read.csv('Electronics_All.csv')
print(dim(electronics))
head(electronics, 1)

```
## Preliminary filtering

We're interested only in reviews which are about SD cards, because our client is a sd card manufacturer. We don't know what are the top selling sd cards, so we can find them by investigating reviews that contain "sd"
in the text.

After we isolate potential candidates, then we can use an amazon search to investigate what the product actually is


```{r}

sd_cards <- electronics[grep("sd", electronics$reviewText), ]
head(sd_cards, 1)

```

If we google or amazon search for asin **1400501466** we find the Barnes & Noble Nook Tablet.

Asin stands for **amazon standard identificaiton number.** We can find out what products in the "sd" area are leading products by counting which asins appear most frequently. We do this by aggregating and then sorting in descending order


```{r}

sd_cards_aggregate <- aggregate(sd_cards['asin'], by = sd_cards['asin'], FUN = length)
colnames(sd_cards_aggregate)<- c('asin', 'count')
head(sd_cards_aggregate[order(-sd_cards_aggregate$count), ], n = 10)



```
After searching the asins online, we see the first and the third are sd cards, and the seventh is our client's sd card.

The second is a solid state drive ('sd' accidentially returned us with 'ssd' as well because 'sd' is a substring of 'ssd').

Next, we can generate some basic summary statistics on the sandisk, kingston, and transcend products to understand what general information exists. 


```{r} 

sandisk<-electronics[electronics$asin == 'B007WTAJTO',]
# a second post processing step is done (LDA cannot work with 0 words in a document)
sandisk<-sandisk[sandisk$reviewText!='', ]

kingston <- electronics[electronics$asin =='B000VX6XL6',]
kingston<-kingston[kingston$reviewText!='', ]

transcend <- electronics[electronics$asin =='B002WE6D44',]
transcend<-transcend[transcend$reviewText!='', ]

# code to print summary statistics is hidden

```
```{r, echo = FALSE}

cat('\n\nsummary statistics for sandisk\n') 
print(paste0('number of reviews is ', nrow(sandisk)))
print(paste0('mean is ', mean(sandisk$overall)))
print(paste0('median is ', median(sandisk$overall)))
print('summary statistics for sandisk') 
print(paste0('standard deviation is ', sd(sandisk$overall)))

cat('\n\nsummary statistics for kingston\n') 
print(paste0('number of reviews is ', nrow(kingston)))
print(paste0('mean is ', mean(kingston$overall)))
print(paste0('median is ', median(kingston$overall)))
print(paste0('standard deviation is ', sd(kingston$overall)))

cat('\n\nsummary statistics for trancend\n') 
print(paste0('number of reviews is ', nrow(transcend)))
print(paste0('mean is ', mean(transcend$overall)))
print(paste0('median is ', median(transcend$overall)))
print(paste0('standard deviation is ', sd(transcend$overall)))


```

## Pivot to Deep Dive


Given that the Sandisk asin has more reviews than the transcend, the VP of Sales says he wants to learn more about the Sandisk product (the leading product) in particular. He wants to know if data science can reveal relative strengths, weakness, and anything else about the two products that can be learned. 

### Step 1: Graph the monthly review average for both sandisk and transcend
#### part a: aggregate average review rating by month
#### part b: visualize over time


```{r, message = FALSE}

require(stringr)
require(lubridate)
require(ggplot2)

# Step 1: Graph the average monthly review average for both sandisk and transcend
  # part a: aggregate average review rating by month

# concatenate the sandisk and transcend dataframes
sandisk_and_transcend <- rbind(sandisk, transcend)
# make a column to identify the maker as Sandisk or Transcend (very useful for ggplot2)
sandisk_and_transcend$maker <- ifelse(sandisk_and_transcend$asin == 'B007WTAJTO', 'Sandisk', 'Transcend')

# wrangle date from reviewtime
# a simpler implementation could probably be done using the anytime package
mon_day_year <- as.data.frame(str_split_fixed(as.character(sandisk_and_transcend$reviewTime), " |, ", n = 3))
mon_day_year <- as.data.frame(sapply(mon_day_year, as.factor))
colnames(mon_day_year)<-c('month', 'day', 'year')

# merge back with the original dataframe
sandisk_and_transcend <- cbind(sandisk_and_transcend, mon_day_year)

# aggregate review average by month and year
sandisk_and_transcend_monthly <- aggregate(sandisk_and_transcend$overall, by = list(sandisk_and_transcend$maker, sandisk_and_transcend$month, sandisk_and_transcend$year), FUN = mean)

# assign column names
colnames(sandisk_and_transcend_monthly)<- c('maker', 'month', 'year', 'overall')

# turn date into a date object and set to first day of the month as default
sandisk_and_transcend_monthly$date <- as.Date(paste('01', sandisk_and_transcend_monthly$month, sandisk_and_transcend_monthly$year, sep = '/'), '%d/%m/%Y')

head(sandisk_and_transcend_monthly)

```
Now that we have the data aggregated and converted into a date format, we can now move to part b, visualizing. 

```{r}
# Step 1: Graph the average monthly review average for both sandisk and transcend
  # part b: visualize over time

ggplot(sandisk_and_transcend_monthly, 
       aes(x=date,y=overall,color=maker)) +
theme(panel.background = element_rect(fill = "white"),
      axis.line=element_line(color = "snow4",
                             size=.5, 
                             linetype='solid'),
      panel.grid.major.y = element_line(size = 0.2, 
                                        linetype='longdash', 
                                        color="snow4"),
      panel.grid.major.x = element_line(size= 0.2, 
                                        linetype='longdash', 
                                        color="snow4"),
      panel.grid.minor.y = element_line(size = .1,
                                        linetype="longdash", 
                                        color="snow4"), #makes minor axis invisible, but able to be customized
      panel.grid.minor.x = element_line(size = .1,
                                        linetype="longdash", 
                                        color="snow4"), #makes minor axis invisible, but able to be customized
      axis.text.x = element_text(angle = 0, hjust = .5, family="Arial", size = 8), 
      axis.title.x = element_blank(), #removed x-axis "Date" title
      axis.title.y = element_text(angle = 90, hjust = 0.5, vjust=0, family = "Arial", size = 8), 
      plot.title = element_text(hjust = 0.5, family = "Arial", size = 12),
)+
scale_y_continuous(expand=c(0,0), lim = c(3.75, 5.25))+
geom_line(stat="identity")+
ylab("Star Score")+
ggtitle("Average Monthy Review Score for Sandisk and Transcend SD Cards")+
labs(caption= "")

```

### Step 2: Find which words are highest correlated with positive reviews and negative reviews for both sandisk and transcend, and visualize using wordclouds
#### part a: use the tm package to process the text and create a dtm
#### part b: calculate correlation of words with the overall star score
#### part c: visualize top words using word clouds
#### part d: repeat for other company


```{r, , message = FALSE, warning=FALSE}
# Step 2: Find which words are highest correlated with positive reviews and negative reviews for both sandisk and transcend, and visualize using wordclouds
  # part a: use the tm package to process the text and create a dtm

require(tm)
require(textstem)

# uncomment the print steps to see what happens at each stage of the pipeline
reviews_sandisk <- Corpus(VectorSource(sandisk$reviewText))
#print(writeLines(as.character(reviews_sandisk[5])))
reviews_sandisk<-tm_map(reviews_sandisk, content_transformer(tolower))
#print(writeLines(as.character(reviews_sandisk[5])))
reviews_sandisk<-tm_map(reviews_sandisk, removeWords, stopwords("english"))
#print(writeLines(as.character(reviews_sandisk[5])))
reviews_sandisk<-tm_map(reviews_sandisk, lemmatize_strings)
#print(writeLines(as.character(reviews_sandisk[5])))
reviews_sandisk<-tm_map(reviews_sandisk, stripWhitespace)
print(writeLines(as.character(reviews_sandisk[5])))

# create dtm
reviews_dtm_sandisk<-DocumentTermMatrix(reviews_sandisk)


```
Now that we have a dtm, we can now find the correlations between the word counts and overall star rating. The easiest (albeit highly inefficient) way seems to be to append the star score column to the dtm, calculate a correlation matrix, and then subset the star score column of the correlation matrix. 
```{r, warning=FALSE}

# Step 2: Find which words are highest correlated with positive reviews and negative reviews for both sandisk and transcend, and visualize using wordclouds
  # part b: calculate correlation of words with the overall star score

# append the review score
reviews_dtm_matrix_sandisk<-cbind(sandisk$overall, as.matrix(reviews_dtm))
colnames(reviews_dtm_matrix_sandisk)[1] <- 'overall_star_rating'
# calcualte a correlation matrix
correlation_matrix_sandisk<-cor(reviews_dtm_matrix_sandisk)
correlation_table_sandisk<- as.data.frame(as.table(correlation_matrix_sandisk))
# filter out only the correlations between word frequencies and the target variable
correlation_table_sandisk<-correlation_table_sandisk[correlation_table_sandisk$Var1 == 'overall_star_rating' & correlation_table_sandisk$Var2 != 'overall_star_rating', ]
# sort correlations in descending order
correlation_table_sandisk<-correlation_table_sandisk[order(-correlation_table_sandisk$Freq), c('Var2', 'Freq')]
colnames(correlation_table_sandisk)<-c('word', 'correlation')
correlation_table_positives_sandisk <- correlation_table_sandisk[1:20, ]
correlation_table_negatives_sandisk <- correlation_table_sandisk[(nrow(correlation_table_sandisk)-20):(nrow(correlation_table_sandisk)), ]

print(rbind(correlation_table_positives_sandisk,correlation_table_negatives_sandisk ))


```

```{r, message = FALSE}

# Step 2: Find which words are highest correlated with positive reviews and negative reviews for both sandisk and transcend, and visualize using wordclouds
  # part c: visualize top words using word clouds

require(wordcloud)
require(RColorBrewer)


pal<-brewer.pal(9, 'PuRd')
wordcloud(correlation_table_negatives_sandisk$word, (correlation_table_negatives_sandisk$correlation*-100), random.color = F, colors = pal)

pal<-brewer.pal(9, 'BuGn')

wordcloud(correlation_table_positives_sandisk$word, (correlation_table_positives_sandisk$correlation*100), random.color = F, colors = pal)


```
Now we'll repeat the same analysis for transcend.
```{r, warning=FALSE}

# Step 2: Find which words are highest correlated with positive reviews and negative reviews for both sandisk and transcend, and visualize using wordclouds
  # part d: repeat for other company

# perform the same analysis now for our client transcend

reviews_transcend <- Corpus(VectorSource(transcend$reviewText))
reviews_transcend<-tm_map(reviews_transcend, content_transformer(tolower))
reviews_transcend<-tm_map(reviews_transcend, removeWords, stopwords("english"))
reviews_transcend<-tm_map(reviews_transcend, lemmatize_strings)
reviews_transcend<-tm_map(reviews_transcend, stripWhitespace)
reviews_dtm_transcend<-DocumentTermMatrix(reviews_transcend)
reviews_dtm_matrix_transcend<-cbind(sandisk_and_transcend$overall, as.matrix(reviews_dtm_transcend))
colnames(reviews_dtm_matrix_transcend)[1] <- 'overall_star_rating'
correlation_matrix_transcend<-cor(reviews_dtm_matrix_transcend)
correlation_table_transcend<- as.data.frame(as.table(correlation_matrix_transcend))
correlation_table_transcend<-correlation_table_transcend[correlation_table_transcend$Var1 == 'overall_star_rating' & correlation_table_transcend$Var2 != 'overall_star_rating', ]
correlation_table_transcend<-correlation_table_transcend[order(-correlation_table_transcend$Freq), c('Var2', 'Freq')]
colnames(correlation_table_transcend)<-c('word', 'correlation')
correlation_table_positives_transcend <- correlation_table_transcend[1:20, ]
correlation_table_negatives_transcend <- correlation_table_transcend[(nrow(correlation_table_transcend)-20):(nrow(correlation_table_transcend)), ]

pal<-brewer.pal(9, 'PuRd')
wordcloud(correlation_table_negatives_transcend$word, (correlation_table_negatives_transcend$correlation*-100), random.color = F, colors = pal)

pal<-brewer.pal(9, 'BuGn')

wordcloud(correlation_table_positives_transcend$word, (correlation_table_positives_transcend$correlation*100), random.color = F, colors = pal)


```


### Step 3: Fit LDA model and see performance by topic 
#### part a: Fit LDA Model (first merge dataframes of both, then fit model)
#### part b: Determine each topic's meaning
#### part c: aggregate the gamma matrix (document topic distributions) by each company
#### part d: Use bar charts to visualize topic frequency differences between the two competitors 

First use a dataframe that contains the row-binded results of both sandisk and transcend reviews. Perform the same text processing, and then fit an LDA model. 

From tinkering, **Gibbs Sampling** yielded better results. 

```{r, message=FALSE, warning=FALSE}
require(topicmodels)
# Step 3: Fit LDA model and see performance by topic 
  # part a: Fit LDA Model
  

reviews<- Corpus(VectorSource(sandisk_and_transcend$reviewText))
reviews<-tm_map(reviews, content_transformer(tolower))
reviews<-tm_map(reviews, removeWords, stopwords("english"))
reviews<-tm_map(reviews, lemmatize_strings)
reviews<-tm_map(reviews, stripWhitespace)
reviews_dtm<-DocumentTermMatrix(reviews)


lda <- LDA(reviews_dtm, k = 7, method = "Gibbs", control = list(seed = 1234))

```
For class, students should use the simple terms function which seems to return top terms from the beta matrix. I experimented with normalizing by the sum of all probabilities. 
```{r}

# Step 3: Fit LDA model and see performance by topic 
  # part b: Determine each topic's meaning

terms(lda, 10)

 # this is a different method to find top words, probably too complex for our class, ctrl shift c to use

 probs <- exp(lda@beta)
 probs_normalized <- probs / colSums(probs)
 terms <- lda@terms

 topic_df<-data.frame(matrix(ncol = 7, nrow = 20))
 colnames(topic_df) <- c('Topic1',
                    'Topic2',
                    'Topic3',
                    'Topic4',
                    'Topic5',
                    'Topic6',
                    'Topic7')

 for (i in 1:nrow(probs_normalized)){
   term_vector<-probs_normalized[i,]
   top_ten_idx<-order(term_vector, decreasing = T)[1:20]
   top_terms<-terms[top_ten_idx]
   topic_df[, i]<-top_terms

 }

topic_df


```
Determine the topic names from the information given. Topic 5 seems both good and bad, so we'll keep it "uncertain." We then should average the topic probabilities by each maker's reviews to find out if customer reviews tend to fall in one or another category. 

```{r}


# record the topic names
topic_names<-c('Performance1', 'Usage', 'Performance2', 'Problems', 'Uncertain', 'Performance3', 'Positive' )

# Step 3: Fit LDA model and see performance by topic 
  # part c: aggregate the gamma matrix (document topic distributions) by each company

# extract the topic distributions for sandisk from the gamma matrix
sandisk_docs<-lda@gamma[which(sandisk_and_transcend$maker=="Sandisk"),]

# calculate the average of the topic distributions for sandisk reviews
sandisk_topic_distribution<-colSums(sandisk_docs)/nrow(sandisk_docs)

sandisk_topic_distribution

# extract the topic distributions for transcend from the gamma matrix
transcend_docs<-lda@gamma[which(sandisk_and_transcend$maker=="Transcend"),]

# calculate the average of the topic distributions for transcend reviews
transcend_topic_distribution<-colSums(transcend_docs)/nrow(transcend_docs)

transcend_topic_distribution


```
Now let's visualize this data side by side using ggplot2
```{r}

# Step 3: Fit LDA model and see performance by topic 
  # part d: Use bar charts to visualize topic frequency differences between the two competitors


# make a dataframe for visualization purposes
Topics <- rep(topic_names, 2)
Maker <- c(rep('Sandisk', 7), rep('Transcend', 7))
Topic_Distributions <- c(sandisk_topic_distribution, transcend_topic_distribution)

company_topic_distributions<-cbind.data.frame(Topics, Maker, Topic_Distributions)

colnames(company_topic_distributions)<-c('Topics', 'Maker', 'Topic_Distributions')

# visualize

company_topic_distributions%>%
  ggplot(aes(x = Topics, y=Topic_Distributions, fill = Maker)) +
  theme(panel.background = element_rect(fill = "white"),
        axis.line=element_line(color = "powderblue",
                               size=.5, 
                               linetype='solid'),
        panel.grid.major.y = element_line(size = 0.2, 
                                          linetype='longdash', 
                                          color="powderblue"),
        panel.grid.major.x = element_line(size= 0.2, 
                                          linetype='longdash', 
                                          color="powderblue"),
        panel.grid.minor.y = element_line(size = .1,
                                          linetype="longdash", 
                                          color="powderblue"), #makes minor axis invisible, but able to be customized
        panel.grid.minor.x = element_line(size = .1,
                                          linetype="longdash", 
                                          color="powderblue"), #makes minor axis invisible, but able to be customized
        axis.text.x = element_text(angle = 0, hjust = .5, family="Arial", size = 8), 
        axis.title.x = element_blank(), #removed x-axis "Date" title
        axis.title.y = element_text(angle = 90, hjust = 0.5, vjust=0, family = "Arial", size = 8), 
        plot.title = element_text(hjust = 0.5, family = "Arial", size = 12),
        
  )+
  scale_y_continuous(expand=c(0,0), lim = c(0,.33))+
  geom_bar(stat = 'identity', position = "dodge")+
  ylab("Topic Frequency")+
  ggtitle("Topic Distributions by Competitor")+
  labs(caption= "")

```
The last code block here is un-implemented code to make seeded LDA topic modeling, unfortunately not enough time to try to debug the issues with the simple triplet matrix ! Just not enough documentation is out there. 
```{r}


# topic 1 - back, issue, problem, crap, slow, unreliable
# topic 2 - fast, reliable, smoothly, rapidly, consistent, durable
# topic 3 - memory, large, hold, storage, plenty, capacity, 
# topic 4 - great, recommend, happy, love, perfect, deal
# topic 5
# topic 6
# topic 7

# was experimenting with setting seed words, but unable to get working in time

# 
# seed_words<-c('back', 'issue', 'problem', 'crap', 'error', 'corrput', 
#               'fast', 'reliable', 'smoothly', 'rapidly', 'consistent', 'durable', 
#               'memory', 'large', 'hold', 'storage', 'plenty', 'capacity', 
#               'great', 'recommend', 'happy', 'love', 'perfect', 'deal')
# 
# 
# which(reviews_dtm[['dimnames']]$Terms == 'back') 
# 
# seed_words_idx<-unname(sapply(seed_words, function(x) which(reviews_dtm[['dimnames']]$Terms == x )))
# eye <- rep(1:4, each = 6)
# jay <- sapply(seed_words, function(x) which(reviews_dtm[['dimnames']]$Terms == x ))
# length(i)
# length(j)
# ncol(reviews_dtm)
# require(slam)
# ?simple_triplet_matrix
# deltaS <- simple_triplet_matrix(i = eye, j = jay, v = rep(500, 24),nrow = 7, ncol = ncol(reviews_dtm))
# 
# ?simple_triplet_matrix
# 
# ldaS <- LDA(AssociatedPress, k = 6, method = "Gibbs", seedwords = deltaS, 
#             control = list(alpha = 0.1, best = TRUE,
#                            verbose = 500, burnin = 500, iter = 100, thin = 100, prefix = character()))
# 


```

